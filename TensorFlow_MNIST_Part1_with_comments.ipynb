{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" of deep learning because for most students it is the first deep learning algorithm they see.\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition.\n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# TensorFLow includes a data provider for MNIST that we'll use.\n",
    "# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using\n",
    "# pip install tensorflow-datasets \n",
    "# or\n",
    "# conda install tensorflow-datasets\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# these datasets will be stored in C:\\Users\\*USERNAME*\\tensorflow_datasets\\...\n",
    "# the first time you download a dataset, it is stored in the respective folder \n",
    "# every other time, it is automatically loading the copy on your computer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "That's where we load and preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember the comment from above\n",
    "# these datasets will be stored in C:\\Users\\*USERNAME*\\tensorflow_datasets\\...\n",
    "# the first time you download a dataset, it is stored in the respective folder \n",
    "# every other time, it is automatically loading the copy on your computer \n",
    "\n",
    "# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) \n",
    "# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument\n",
    "# there are other arguments we can specify, which we can find useful\n",
    "# mnist_dataset = tfds.load(name='mnist', as_supervised=True)\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
    "# we will use this information a bit below and we will store it in mnist_info\n",
    "\n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n",
    "# alternatively, as_supervised=False, would return a dictionary\n",
    "# obviously we prefer to have our inputs and targets separated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train,mnist_test = mnist_dataset[\"train\"], mnist_dataset[\"test\"]\n",
    "# there is no validation dataset within mnist. its 60.000train and 10.000 test\n",
    "# we get the validation from the train, 10% of it\n",
    "#validation dataset is used to make sure our parameters, weights and biases do not overfit\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits[\"train\"].num_examples #create validation dataset\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64) #cast the value of samples into integer, prevent potential issues\n",
    "\n",
    "num_test_samples = mnist_info.splits[\"test\"].num_examples #store the test samples into a variable\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "#create a function to scale the inputs\n",
    "\n",
    "def scale(image,label):\n",
    "    image = tf.cast(image,tf.float32)\n",
    "    #mnist images contain color levels from 0 to 255\n",
    "    image /=255. # dot at the end is becasue we want a float value\n",
    "    return image, label\n",
    "\n",
    "# the method .map() allows us to apply a custom transformation to a given dataset\n",
    "# we have already decided that we will get the validation data from mnist_train\n",
    "sclaed_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "#scale the test data\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "#shuffling the data - the same information but in a different order\n",
    "#its better the shuffle batch data, it needs to be randomly distributed\n",
    "\n",
    "buffer_size = 10000 #tensorflow take samples of 10k\n",
    "\n",
    "shuffled_train_and_validation_data = sclaed_train_and_validation_data.shuffle(buffer_size)\n",
    "\n",
    "#create validation dataset\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_data = train_data.batch(batch_size)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "#validation data must have the same shape and properties as train/test data\n",
    "#mnist is iterable\n",
    "#iter creates an object which can be iterated on element at a time, like in a for loop\n",
    "#next loads the next element of an iterable object\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10 #one for every digit\n",
    "hidden_layer_size = 200 #assume all of them same\n",
    "\n",
    "#tf.keras.layers.Flatten(original shape) - transforms a tensor into a vector\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"),#calculates the dot product of inputs and the weights and adds the bias\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"), # because we plan to have 2 hidden layers, we can stack more layers if we want\n",
    "    tf.keras.layers.Dense(output_size, activation=\"softmax\")\n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choosing the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify optimizer and the loss with model.optimizer\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 3s - loss: 0.2748 - accuracy: 0.9193 - val_loss: 0.1352 - val_accuracy: 0.9590 - 3s/epoch - 5ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 2s - loss: 0.1057 - accuracy: 0.9687 - val_loss: 0.0812 - val_accuracy: 0.9757 - 2s/epoch - 4ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 2s - loss: 0.0718 - accuracy: 0.9784 - val_loss: 0.0618 - val_accuracy: 0.9822 - 2s/epoch - 4ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0527 - accuracy: 0.9834 - val_loss: 0.0471 - val_accuracy: 0.9860 - 3s/epoch - 5ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 2s - loss: 0.0388 - accuracy: 0.9876 - val_loss: 0.0532 - val_accuracy: 0.9818 - 2s/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1530859c5b0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model.fit(train_data,epochs=num_epochs,validation_data=(validation_inputs,validation_targets), verbose=2)\n",
    "# verbose=2 generates only the most important info in each epoch\n",
    "\n",
    "# 1-at the beginnig of each epoch, the training loss will be set to 0\n",
    "# 2-the algorihm iterate over number of batches, all from train_data\n",
    "# 3-the weights and biases will be updated as many times there are batch_size\n",
    "# 4-we get a value for the loss func, indicates how the training is going\n",
    "# 5-we also see accuarcy\n",
    "# 6-at the end of each eopch, the algorithm forward propagate the whole validation set\n",
    "# 7-when we reach max number of epochs, the training is over\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0792 - accuracy: 0.9761\n",
      "Test loss:0.08 and Test accuracy: 97.61%\n"
     ]
    }
   ],
   "source": [
    "#test the model on test dataset to get actual accuracy, the one we have above is on validation dataset\n",
    "\n",
    "test_loss,test_accuracy = model.evaluate(test_data)\n",
    "print(\"Test loss:{0:.2f} and Test accuracy: {1:.2f}%\".format(test_loss,test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latest Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we test the model, conceptually, we are no longer allowed to change it.\n",
    "# If you start changing the model after this point, the test data will no longer be a data set the model has never seen.\n",
    "# You would have feedback that it has around 97.5% accuracy.\n",
    "# With this particular configuration, the main point of the test data set is to simulate model deployment.\n",
    "# If we get 50% or 60% testing accuracy, we will know for sure that our model has overfit and it will fail miserably in real life.\n",
    "# However, getting a value very close to the validation accuracy shows that we have not overfitted.\n",
    "# Finally, the test accuracy is the accuracy we expect to observe if we deploy the model in the real world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3055225c4516c39aad8f22a26de8830b0fa5a38521c8908c81279ed803dd281b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
